{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c",
   "metadata": {
    "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Код во многом опирается на книгу <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> за авторством <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
   "metadata": {
    "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
    "tags": []
   },
   "source": [
    "# Small Qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d",
   "metadata": {
    "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d"
   },
   "source": [
    "- Этот notebook намеренно минималистичен и посвящен реализации небольшой версии Qwen3 0.6B, 1.7B, 4B, 8B, and 32B. Больше информации о модели можно получить по ссылкам:\n",
    "  - [Qwen3: Think Deeper, Act Faster](https://qwenlm.github.io/blog/qwen3/)\n",
    "  - [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388) \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/qwen/qwen-overview.webp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c201adb-747e-437b-9a62-442802941e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r sqwen_req.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
    "outputId": "4f762354-e0a3-4cc2-e5d4-e61a227a202c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.33.4\n",
      "tokenizers version: 0.21.2\n",
      "torch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tokenizers\",       # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e96fbb-8e16-4f6d-835f-c6159321280b",
   "metadata": {},
   "source": [
    "- Реализация поддерживает reasoning (\"chain of thoughts\"), который активируется следующим флагом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a90338-624a-4706-aa55-6b4358070194",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
   "metadata": {
    "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
    "tags": []
   },
   "source": [
    "# 1. Architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
   "metadata": {
    "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Инициализация двух параллельных ветвей для гейтированного механизма\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "        # Финалный слой для проекции обратно в пространство эмбеддингов\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "\n",
    "        # Гейтированный механизм: SiLU-активация работает как \"выключатель\" для информационных потоков\n",
    "        # Умножение выходов fc1 и fc2 создает адактивный фильтр для входных данных\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "\n",
    "        # Компенсация роста дисперсии после мультипликативной операции\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42884c-1580-40f4-831b-e863f6046a05",
   "metadata": {},
   "source": [
    "### Пояснения к ключевым моментам:\n",
    "1. **Гейтированный механизм**  \n",
    "   Использование двух параллельных ветвей (`fc1` и `fc2`) с последующим умножением реализует механизм адактивной фильтрации, где:\n",
    "   - `fc1` формирует \"ворота\" через SiLU-активацию (логистический выбор информации)\n",
    "   - `fc2` предоставляет \"кандидатные значения\" для трансформации\n",
    "   Такая архитектура эффективнее стандартного `Linear -> ReLU -> Linear` для задач с разреженными паттернами.\n",
    "\n",
    "2. **Отсутствие смещений (bias=False)**  \n",
    "   Сознательное решение для:\n",
    "   - Снижения переобучения\n",
    "   - Упрощения динамики обучения\n",
    "   - Совместимости с техниками нормализации (LayerNorm обычно компенсирует отсутствие bias)\n",
    "\n",
    "3. **Мультипликативное взаимодействие**  \n",
    "   Операция `silu(a) * b`:\n",
    "   - Добавляет нелинейность выше 2-го порядка\n",
    "   - Позволяет моделировать сложные зависимости между нейронами\n",
    "   - Эффективнее классических активаций для языковых задач (аналогично Gated Linear Units)\n",
    "\n",
    "4. **Типизация через cfg[\"dtype\"]**  \n",
    "   Автоматическое поддержание выбранного типа данных (float16/float32) на всех слоях:\n",
    "   - Критично для mixed-precision training\n",
    "   - Гарантирует согласованность вычислений\n",
    "\n",
    "5. **Семантика размерностей**  \n",
    "   `emb_dim` -> `hidden_dim` -> `emb_dim`:\n",
    "   - Сквозной принцип (residual path friendly)\n",
    "   - Соответствие оригинальной схеме Transformer'а\n",
    "   - Сохранение формы тензора для последующих операций сложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56715760-37e1-433e-89da-04864c139a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        \n",
    "        # Параметр масштабирования (обязательный)\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        \n",
    "        # Опциональный параметр смещения (включается через bias=True)\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Сохраняем исходный тип для корректного восстановления\n",
    "        input_dtype = x.dtype\n",
    "        \n",
    "        # Режим совместимости с Qwen3: вычисления в float32 для стабильности\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        # Вычисление RMS по последнему измерению:\n",
    "        # 1. Квадраты элементов (pow(2))\n",
    "        # 2. Усреднение по последней оси (keepdim сохраняет размерность)\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Нормализация:\n",
    "        # 1. Обратный квадратный корень дисперсии (rsqrt = 1/sqrt(v))\n",
    "        # 2. Масштабирование исходных значений\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        \n",
    "        # Применение обучаемого масштаба\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        # Добавление смещения (если включено в конфигурации)\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "\n",
    "        # Восстановление исходного типа данных\n",
    "        return norm_x.to(input_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffcbb33-1629-496f-9460-a6c5408e0e0e",
   "metadata": {},
   "source": [
    "### Ключевые особенности реализации:\n",
    "1. **Совместимость с Qwen3**  \n",
    "   Флаг `qwen3_compatible` включает приведение к float32 для:\n",
    "   - Повышения численной стабильности\n",
    "   - Гарантии бит-в-бит соответствия оригинальной реализации\n",
    "   - Автоматического восстановления исходного типа данных после вычислений\n",
    "\n",
    "2. **RMS-нормализация**  \n",
    "   Формула `x * (1 / sqrt(mean(x^2) + eps))`:\n",
    "   - Отличается от LayerNorm отсутствием центрирования (вычитания среднего)\n",
    "   - Эффективнее вычислительно (меньше операций)\n",
    "   - Показала лучшую сходимость в больших языковых моделях\n",
    "\n",
    "3. **Опциональное смещение**  \n",
    "   Параметр `bias` контролирует:\n",
    "   - Добавление обучаемого параметра `shift`\n",
    "   - Совместимость с вариантами архитектур (например, LLaMA не использует смещение)\n",
    "   - Гибкость конфигурации без изменения кода\n",
    "\n",
    "4. **Управление типами данных**  \n",
    "   Явное сохранение/восстановление типа:\n",
    "   - Позволяет работать с mixed-precision (FP16/BF16)\n",
    "   - Предотвращает потерю точности в режиме совместимости\n",
    "   - Автоматически адаптируется под входной тензор\n",
    "\n",
    "5. **Стабильность вычислений**  \n",
    "   Параметр `eps` (ε) решает проблемы:\n",
    "   - Деления на ноль при нулевой дисперсии\n",
    "   - Численной нестабильности для малых значений дисперсии\n",
    "   - Рекомендуемое значение 1e-6 балансирует точность и устойчивость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
   "metadata": {
    "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
   },
   "outputs": [],
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    # Проверка четности размерности головы (необходимо для парного разбиения)\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Вычисление обратных частот для вращения:\n",
    "    # 1. Создание последовательности [0, 2, 4, ..., head_dim-2]\n",
    "    # 2. Нормализация индексов по размерности головы\n",
    "    # 3. Экспоненциальное затухание частот с базой theta_base\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Генерация позиционных индексов [0, 1, 2, ..., context_length-1]\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Создание вращательных углов:\n",
    "    # 1. Внешнее произведение позиций и частот (positions[:, None] * inv_freq[None, :])\n",
    "    # 2. Результат - матрица размером (context_length, head_dim//2) \n",
    "    angles = positions[:, None] * inv_freq[None, :]\n",
    "\n",
    "    # Дублирование углов для полного покрытия head_dim:\n",
    "    # [θ0, θ1, ..., θ_{d/2-1}] -> [θ0, θ1, ..., θ_{d/2-1}, θ0, θ1, ..., θ_{d/2-1}]\n",
    "    angles = torch.cat([angles, angles], dim=1)\n",
    "\n",
    "    # Предварительное вычисление косинусов и синусов:\n",
    "    # - Эти тензоры будут кэшироваться для эффективности\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # Размерность входного тензора: [batch, heads, seq_len, head_dim]\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Разделение скрытого состояния на две половины:\n",
    "    # x1 = [x0, x1, ..., x_{d/2-1}]\n",
    "    # x2 = [x_{d/2}, x_{d/2+1}, ..., x_{d-1}]\n",
    "    x1 = x[..., : head_dim // 2]\n",
    "    x2 = x[..., head_dim // 2 :]\n",
    "\n",
    "    # Подготовка вращательных матриц:\n",
    "    # 1. Срез по актуальной длине последовательности\n",
    "    # 2. Добавление размерностей для broadcasing [1, 1, seq_len, head_dim]\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Создание вращаемой компоненты:\n",
    "    # rotated = [-x2, x1] - перестановка и смена знака\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    # Применение ротационного преобразования:\n",
    "    # x_rot = x ⊗ cos(θ) + rotated ⊗ sin(θ)\n",
    "    # Эквивалентно комплексному вращению: (x1 + i·x2) ⊗ (cosθ + i·sinθ)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # Восстановление оригинального типа данных:\n",
    "    # - После операций с float32 возвращаем исходный dtype (float16/bf16)\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ebd31b-73c5-4f72-86ce-96082f1a24a4",
   "metadata": {},
   "source": [
    "### Ключевые пояснения:\n",
    "\n",
    "1. **Физика RoPE**\n",
    "   - Реализует вращение векторов в многомерном пространстве\n",
    "   - Аналогично умножению комплексных чисел: (x1 + ix2) * (cosθ + isinθ)\n",
    "   - Сохраняет относительные позиционные расстояния (инвариантность к сдвигу)\n",
    "\n",
    "2. **Оптимизации**\n",
    "   - Предварительный расчет cos/sin: вычисляются один раз при инициализации\n",
    "   - Подготовка углов: оптимизированная формула без экспонент\n",
    "   - Broadcasting: эффективное применение к пакетам данных\n",
    "\n",
    "3. **Особенности реализации**\n",
    "   - Четное разбиение: обязательное требование для парного вращения\n",
    "   - Обратные частоты: гарантируют плавное затухание углов по измерениям\n",
    "   - Сохранение типа: критично для mixed-precision обучения\n",
    "\n",
    "4. **Почему работает?**\n",
    "   - Кодирует позиции через вращение векторов запросов/ключей\n",
    "   - Сохраняет скалярное произведение только для близких позиций\n",
    "   - Автоматически учитывает относительное расстояние между токенами\n",
    "\n",
    "5. **Преимущества перед другими методами**\n",
    "   - Относительное позиционирование (не абсолютные позиции)\n",
    "   - Расширяемость на большие контексты\n",
    "   - Сохранение длины векторов (норма не меняется)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
   "metadata": {
    "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
   },
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Проверка делимости количества голов на группы\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        # Размер группы запросов\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            # Автоматический расчет размерности головы\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        # Выходная размерность всех голов\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        # Проекционные слои для Q, K, V\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        # KV имеют групповую размерность\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        # Финалный проекционный слой\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        # Опциональная нормализация QK\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        # Извлечение размерностей\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Применение проекций\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Решейпинг в тензоры с отдельной размерностью голов\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Опциональная нормализация\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys = self.k_norm(keys)\n",
    "\n",
    "        # Применение ротационных позиционных эмбеддингов\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Расширение K и V для всех голов запросов\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        # Применение маски (заполнение -inf)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        # Softmax с масштабированием\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "\n",
    "        # Аггрегация значений\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        # Финальная проекция\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523bc0ad-0f53-45cf-adea-19dadfc478ae",
   "metadata": {},
   "source": [
    "### Ключевые моменты GroupedQueryAttention:\n",
    "\n",
    "1. **Групповая архитектура запросов**  \n",
    "   Разделение голов на группы ключей-значений (KV) и запросов (Q) позволяет:\n",
    "   - Снизить потребление памяти на 30-50% по сравнению с Multi-Head Attention\n",
    "   - Сохранить качество модели при уменьшении числа параметров\n",
    "   - Каждая группа KV обслуживает несколько голов Q (group_size = num_heads / num_kv_groups)\n",
    "\n",
    "2. **Эффективные проекционные слои**  \n",
    "   Асимметричная проекция Q, K, V:\n",
    "   - Q: полная проекция (num_heads * head_dim) для индивидуальных запросов\n",
    "   - K/V: сокращенная проекция (num_kv_groups * head_dim) для общих ключей и значений\n",
    "   - Значительное уменьшение параметров в K/V проекциях\n",
    "\n",
    "3. **Ротационные позиционные эмбеддинги (RoPE)**  \n",
    "   Применение apply_rope к Q и K:\n",
    "   - Учет позиционной информации через эмбеддинги\n",
    "   - Повышение качества работы с последовательностями различной длины\n",
    "   - Современный стандарт для позиционного кодирования\n",
    "\n",
    "4. **Опциональная нормализация QK**  \n",
    "   Контролируемая параметром qk_norm:\n",
    "   - Стабилизация обучения через RMSNorm запросов и ключей\n",
    "   - Улучшение сходимости модели\n",
    "   - Предотвращение \"взрыва\" градиентов\n",
    "\n",
    "5. **Механизм расширения KV**  \n",
    "   repeat_interleave для ключей и значений:\n",
    "   - Эффективное повторение KV групп для multiple голов запросов\n",
    "   - Минимизация overhead при вычислении внимания\n",
    "   - Сохранение tensor shape для последующих операций\n",
    "\n",
    "6. **Оптимизированное вычисление внимания**  \n",
    "   Стандартная формула с масштабированием и маскированием:\n",
    "   - Масштабирование на sqrt(head_dim) для стабильности softmax\n",
    "   - Поддержка маскирования через замену на -inf\n",
    "   - Оптимизированные матричные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
   "metadata": {
    "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Инициализация группового внимания запросов\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        # Инициализация прямой связи (FeedForward)\n",
    "        self.ff = FeedForward(cfg)\n",
    "        # Нормализация перед вниманием\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        # Нормализация перед прямой связью\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        # shortcut для блока внимания\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x, mask, cos, sin)  # Форма [batch_size, num_tokens, emb_size]\n",
    "        # Возврат исходного входа (residual connection)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # shortcut для блока прямой связи\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # Возврат исходного входа (residual connection)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498b21b-5ef7-4c90-af6e-a97a94ce2dc0",
   "metadata": {},
   "source": [
    "### Ключевые моменты TransformerBlock:\n",
    "\n",
    "1. **Композиционная архитектура**  \n",
    "   Блок объединяет два ключевых компонента трансформера:\n",
    "   - `GroupedQueryAttention`: механизм группового внимания с оптимизированными вычислениями\n",
    "   - `FeedForward`: двухслойная нейронная сеть с нелинейным преобразованием\n",
    "   - Оба компонента работают с одинаковой размерностью `emb_dim`\n",
    "\n",
    "2. **Pre-Normalization схема**  \n",
    "   Применение нормализации ДО операций внимания и FFN:\n",
    "   - `norm1` перед вниманием стабилизирует входные данные\n",
    "   - `norm2` перед FFN обеспечивает согласованность распределения\n",
    "   - Улучшает сходимость и стабильность обучения по сравнению с Post-Norm\n",
    "\n",
    "3. **Residual Connections (остаточные связи)**  \n",
    "   Сохранение и добавление исходного входа после каждой операции:\n",
    "   - `x = x + shortcut` предотвращает затухание градиентов\n",
    "   - Обеспечивает альтернативный путь для распространения градиента\n",
    "   - Позволяет строить глубокие сети без проблем исчезающих градиентов\n",
    "\n",
    "4. **Сквозная передача контекста**  \n",
    "   Единая обработка маски и позиционных эмбеддингов:\n",
    "   - Маска `mask` обеспечивает корректное вычисление внимания\n",
    "   - `cos`/`sin` параметры для RoPE передаются в механизм внимания\n",
    "   - Консистентность размерностей на всех этапах обработки\n",
    "\n",
    "5. **Конфигурируемость через параметры**  \n",
    "   Гибкая настройка через словарь конфигурации `cfg`:\n",
    "   - Единый источник параметров для всех компонентов\n",
    "   - Согласованность типов данных через `dtype`\n",
    "   - Возможность тонкой настройки архитектуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
   "metadata": {
    "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
   },
   "outputs": [],
   "source": [
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusuable utilities\n",
    "        if cfg[\"head_dim\"] is None:\n",
    "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        else:\n",
    "            head_dim = cfg[\"head_dim\"]\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # Forward pass\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        \n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
   "metadata": {
    "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
   },
   "source": [
    "# 2. Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa142fa-b375-4e78-b392-2072ced666f3",
   "metadata": {
    "id": "caa142fa-b375-4e78-b392-2072ced666f3"
   },
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"0.6B\"\n",
    "\n",
    "if CHOOSE_MODEL == \"0.6B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,           # Vocabulary size\n",
    "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
    "        \"emb_dim\": 1024,                 # Embedding dimension\n",
    "        \"n_heads\": 16,                   # Number of attention heads\n",
    "        \"n_layers\": 28,                  # Number of layers\n",
    "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
    "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
    "        \"qk_norm\": True,                 # Whether to normalize queries and values in GQA\n",
    "        \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
    "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
    "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"1.7B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 2152,                 # 2.1x prev\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 28,\n",
    "        \"hidden_dim\": 6452,              # 2.1x prev\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }   \n",
    "\n",
    "elif CHOOSE_MODEL == \"4B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 2690,                 # ~25% larger \n",
    "        \"n_heads\": 32,                   # 2x larger \n",
    "        \"n_layers\": 36,                  # ~29% larger \n",
    "        \"hidden_dim\": 9876,              # ~3x larger \n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }  \n",
    "\n",
    "elif CHOOSE_MODEL == \"8B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 4096,                 # ~60% larger \n",
    "        \"n_heads\": 32,\n",
    "        \"n_layers\": 36,                  # ~26% larger \n",
    "        \"hidden_dim\": 12288,\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    } \n",
    "\n",
    "elif CHOOSE_MODEL == \"14B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 5120,                 # ~25% larger \n",
    "        \"n_heads\": 40,                   # ~25% larger \n",
    "        \"n_layers\": 40,                  # ~11% larger \n",
    "        \"hidden_dim\": 17408,             # ~42% larger \n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    } \n",
    "\n",
    "elif CHOOSE_MODEL == \"32B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 5120,                \n",
    "        \"n_heads\": 64,                   # ~60% larger \n",
    "        \"n_layers\": 64,                  # ~60% larger \n",
    "        \"hidden_dim\": 25600,             # ~47% larger \n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    } \n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"{CHOOSE_MODEL} is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
   "metadata": {
    "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = Qwen3Model(QWEN3_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf86265-4e9d-4024-9ed0-99076944e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aca91d-4bee-45ce-993a-4ec5393abe2b",
   "metadata": {},
   "source": [
    "- A quick check that the forward pass works before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf0a6b7-b688-42c9-966e-c223d34db99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2256, -0.0112, -0.7109,  ...,  0.4355,  0.1216,  1.0781],\n",
       "         [-0.6484,  0.5430, -0.0708,  ..., -0.0664,  0.5352,  0.3145],\n",
       "         [-0.4785, -0.1572,  0.1099,  ..., -0.2217,  0.2295,  0.6289]]],\n",
       "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([1, 2, 3]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
    "outputId": "00d7e983-262e-4c65-f322-f4d999311988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 751,632,384\n",
      "\n",
      "Total number of unique parameters: 596,049,920\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
    "outputId": "65c1a95e-b502-4150-9e2e-da619d9053d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (стандарт PyTorch): 5.64 GB\n",
      "bfloat16: 2.82 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Расчет общего количества элементов в параметре\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Проверка, хранятся ли градиенты для этого параметра\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Расчет размера буферов (непараметрические данные)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Размер в байтах = (Количество элементов) * (Размер элемента в байтах)\n",
    "    # Предполагаем, что параметры и градиенты хранятся в том же типе, что и входные данные\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Конвертация байтов в гигабайты\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (стандарт PyTorch): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
   "metadata": {
    "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172f89f-d301-439f-b809-46169e5f5945",
   "metadata": {
    "id": "c172f89f-d301-439f-b809-46169e5f5945"
   },
   "source": [
    "# 3. Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75166128-5899-4995-9b88-9672e135650e",
   "metadata": {
    "id": "75166128-5899-4995-9b88-9672e135650e"
   },
   "outputs": [],
   "source": [
    "def load_weights_into_qwen(model, param_config, params):\n",
    "    def assign(left, right, tensor_name=\"unknown\"):\n",
    "        if left.shape != right.shape:\n",
    "            raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "        return torch.nn.Parameter(right.clone().detach() if isinstance(right, torch.Tensor) else torch.tensor(right))\n",
    "\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "        block = model.trf_blocks[l]\n",
    "        att = block.att\n",
    "\n",
    "        # Q, K, V projections\n",
    "        att.W_query.weight = assign(\n",
    "            att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        att.W_key.weight = assign(\n",
    "            att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        att.W_value.weight = assign(\n",
    "            att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        att.out_proj.weight = assign(\n",
    "            att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "\n",
    "        # QK norms\n",
    "        if hasattr(att, \"q_norm\") and att.q_norm is not None:\n",
    "            att.q_norm.scale = assign(\n",
    "                att.q_norm.scale,\n",
    "                params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
    "                f\"model.layers.{l}.self_attn.q_norm.weight\"\n",
    "            )\n",
    "        if hasattr(att, \"k_norm\") and att.k_norm is not None:\n",
    "            att.k_norm.scale = assign(\n",
    "                att.k_norm.scale,\n",
    "                params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
    "                f\"model.layers.{l}.self_attn.k_norm.weight\"\n",
    "            )\n",
    "\n",
    "        # Attention layernorm\n",
    "        block.norm1.scale = assign(\n",
    "            block.norm1.scale,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # Feedforward weights\n",
    "        block.ff.fc1.weight = assign(\n",
    "            block.ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        block.ff.fc2.weight = assign(\n",
    "            block.ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        block.ff.fc3.weight = assign(\n",
    "            block.ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        block.norm2.scale = assign(\n",
    "            block.norm2.scale,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # Final normalization and output head\n",
    "    model.final_norm.scale = assign(model.final_norm.scale, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    if \"lm_head.weight\" in params:\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        # Model uses weight tying, hence we reuse the embedding layer weights here\n",
    "        print(\"Model uses weight tying.\")\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9881b6995c3f49dc89e6992fd9ab660b",
      "17a3174e65c54476b2e0d1faf8f011ca",
      "1bbf2e62c0754d1593beb4105a7f1ac1",
      "b82112e1dec645d98aa1c1ba64abcb61",
      "271e2bd6a35e4a8b92de8697f7c0be5f",
      "90a79523187446dfa692723b2e5833a7",
      "431ffb83b8c14bf182f0430e07ea6154",
      "a8f1b72a33dd4b548de23fbd95e0da18",
      "25cc36132d384189acfbecc59483134b",
      "bfd06423ad544218968648016e731a46",
      "d029630b63ff44cf807ade428d2eb421"
     ]
    },
    "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
    "outputId": "55b2f28c-142f-4698-9d23-d27456d3ed6d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "\n",
    "from_hf = False\n",
    "\n",
    "if REASONING:\n",
    "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}\"\n",
    "else:\n",
    "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}-Base\"\n",
    "\n",
    "local_dir = Path(repo_id).parts[-1]\n",
    "\n",
    "if CHOOSE_MODEL == \"0.6B\":\n",
    "    if from_hf:\n",
    "        weights_file = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=\"model.safetensors\",\n",
    "            local_dir=local_dir,\n",
    "        )\n",
    "    else:\n",
    "        weights_file = \"Qwen3-0.6B/model.safetensors\"\n",
    "    weights_dict = load_file(weights_file)\n",
    "else:\n",
    "    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
    "    index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
    "    with open(index_path, \"r\") as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    weights_dict = {}\n",
    "    for filename in set(index[\"weight_map\"].values()):\n",
    "        shard_path = os.path.join(repo_dir, filename)\n",
    "        shard = load_file(shard_path)\n",
    "        weights_dict.update(shard)\n",
    "\n",
    "load_weights_into_qwen(model, QWEN3_CONFIG, weights_dict)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b345491-3510-4397-92d3-cd0a3fa3deee",
   "metadata": {},
   "source": [
    "# 4. Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b68ab489-48e5-471e-a814-56cda2d60f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "class Qwen3Tokenizer():\n",
    "    def __init__(self, tokenizer_file_path=\"tokenizer.json\", repo_id=None, add_generation_prompt=False, add_thinking=False):\n",
    "        self.tokenizer_file_path = tokenizer_file_path\n",
    "        self.add_generation_prompt = add_generation_prompt\n",
    "        self.add_thinking = add_thinking\n",
    "\n",
    "        tokenizer_file_path_obj = Path(tokenizer_file_path)\n",
    "        if not tokenizer_file_path_obj.is_file() and repo_id is not None:\n",
    "            _ = hf_hub_download(\n",
    "                repo_id=repo_id,\n",
    "                filename=str(tokenizer_file_path_obj.name),\n",
    "                local_dir=str(tokenizer_file_path_obj.parent.name)\n",
    "            )\n",
    "        self.tokenizer = Tokenizer.from_file(tokenizer_file_path)\n",
    "\n",
    "    def encode(self, prompt):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]  \n",
    "        formatted_prompt = self.format_qwen_chat(\n",
    "            messages,\n",
    "            add_generation_prompt=self.add_generation_prompt,\n",
    "            add_thinking=self.add_thinking\n",
    "        )\n",
    "        return self.tokenizer.encode(formatted_prompt).ids\n",
    "                \n",
    "    def decode(self, token_ids):\n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_qwen_chat(messages, add_generation_prompt=False, add_thinking=False):\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            prompt += f\"<|im_start|>{msg['role']}\\n{msg['content']}<|im_end|>\\n\"\n",
    "        if add_generation_prompt:\n",
    "            prompt += \"<|im_start|>assistant\"\n",
    "            if not add_thinking:\n",
    "                prompt += \"<|think>\\n\\n<|/think>\\n\\n\"\n",
    "            else:\n",
    "                prompt += \"\\n\"    \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6df8bc-7308-468e-93ce-2d5529ea7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REASONING:\n",
    "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}/tokenizer.json\"\n",
    "else:\n",
    "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}-Base/tokenizer.json\"\n",
    "\n",
    "tokenizer = Qwen3Tokenizer(\n",
    "    tokenizer_file_path=tokenizer_file_path,\n",
    "    repo_id=None,\n",
    "    add_generation_prompt=REASONING,\n",
    "    add_thinking=REASONING\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d07df1-4401-4792-b549-7c4cc5632323",
   "metadata": {
    "id": "57d07df1-4401-4792-b549-7c4cc5632323"
   },
   "source": [
    "# 5. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1946b534-e3af-431a-a222-391a60bfa892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nОпиши, как работает chain of thoughts<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Опиши, как работает chain of thoughts\"\n",
    "\n",
    "input_token_ids = tokenizer.encode(prompt)\n",
    "text = tokenizer.decode(input_token_ids)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5",
   "metadata": {
    "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5"
   },
   "outputs": [],
   "source": [
    "# Identical function from chapter 5\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.2, top_k=None, eos_id=None):\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(-torch.inf).to(logits.device), logits)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if eos_id is not None and idx_next.item() == eos_id:\n",
    "            break  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
   "metadata": {
    "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 25.36 sec\n",
      "Max memory allocated: 3.15 GB\n",
      "<|im_start|>user\n",
      "Опиши, как работает chain of thoughts<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Хорошо, пользователь спрашивает о том, как работает chain of thought. Нужно объяснить, что это такое, но сначала понять, что именно он хочет. Chain of thought — это логическая последовательность мысли, которая помогает в решении задач. Возможно, он хочет понять, как это применяется в разных областях, или как это связано с алгоритмами.\n",
      "\n",
      "Надо начать с определения chain of thought. Может, стоит уточнить, что оно включает в себя логические шаги, шаги в решении задачи. Затем объяснить, как это работает, с примерами. Например, в математике, когда решают уравнения, мы следуем шагам. В программировании, когда мы кодируем, мы следуем шагам. В логике, когда мы разбираем задачи.\n",
      "\n",
      "Может, стоит добавить, что chain of thought помогает в том, чтобы увидеть все возможные варианты, и выбрать наиболее подходящий. Также важно упомянуть, что это не только для задач, но и для решения проблем, даже в сложных ситуациях.\n",
      "\n",
      "Нужно проверить, не упустил ли я какие-то важные моменты. Возможно, стоит добавить пример из разных областей, чтобы пользователь мог лучше понять. Также важно сделать объяснение понятным и без технических терминов, если это возможно.\n",
      "</think>\n",
      "\n",
      "Chain of thought — это логическая последовательность мыслей, которая помогает в решении задач, решении проблем и в понимании мира. Основной принцип заключается в том, чтобы следовать шагам, которые позволяют переходить от одного шага к другому, и в итоге достигнуть нужного результата.\n",
      "\n",
      "### Как работает chain of thought:\n",
      "1. **Сначала определяется цель**: Нужно понять, что нужно найти, что нужно сделать, или что нужно решить.\n",
      "2. **Проводятся шаги**: Сначала идёт по логике, следуя правилам, которые помогают в решении. Например, в математике — по уравнениям, в...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "torch.manual_seed(21)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "output_token_ids = generate(\n",
    "    model=model,\n",
    "    idx=torch.tensor(input_token_ids, device=device).unsqueeze(0),\n",
    "    max_new_tokens=512,\n",
    "    context_size=QWEN3_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.01\n",
    ")\n",
    "\n",
    "print(f\"Time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")\n",
    "\n",
    "output_text = tokenizer.decode(output_token_ids.squeeze(0).tolist())\n",
    "\n",
    "print(output_text + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5ac5a-7e50-476a-95f9-ea462960d844",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c4edd-83cd-414d-9971-9235790fe8bf",
   "metadata": {},
   "source": [
    "Получилось реализовать LLM с chain of thoughts для более глубокого понимания внутренего устройства LLM моделей такого типа."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
